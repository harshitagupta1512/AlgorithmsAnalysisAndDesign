# Week 7
## Lecture 1
----
## Dynamic Programming

### Problem 6 - Shortest Reliable Path

**Problem**
> Given a directed, weighted graph G(V, E), consider two node `s` and `t`, we want to find the `shortest path` from s to t that uses `atmost k` edges.

>Number of paths between two nodes can grow exponentially with increase in number of edges in the graph.\
A path between two nodes will not hop more than `E` - total number of edges in the graph.

**Subproblem**

>`dist(v, i)` = length of the shortest path from `s` to `v` that uses `i` edges. 

**Optimal Substructure Property - How do we express this in terms of smaller subproblems ?**

>Let the last edge (that enters v) in the path found for $dist(i, v)$ be `(u, v)`

$dist(v, i)$ = $min$ { $dist(u, i - 1) + l(u, v)$ } where $u$ belongs to the set of all possible in-neighbours of $v$. (i.e. (u, v) is a edge directed from u to v).

**Note**
>`Memoization` is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again.\
>Memoization ensures that a method doesn't run for the same inputs more than once by keeping a record of the results for the given inputs.\
> Dynamic Programming is similar to Memoization though it uses iteration where memoization uses recursion

>**Divide and Conquer** states that a problem can be divided into sub-problems. When these subproblems follow the optimal substructure property (if an optimal solution can be constructed from optimal solutions of it's subproblems) and there is considerable overlapping between the sub-problems we use DP. The main advantage of DP is that it makes the code simpler and cleaner. The runtime may or may not be as less as greedy algorithms. Greedy algorithm is a special case of DP where the subproblems also satisfy the greedy choice property(a global optimum can be reached by selecting the local optimums).  
### Problem 7 - Floyd Warshall Algorithm

>Given a graph $G(V,E)$

>**Dijkstra's** Algorithm is a single-source shortest path algorithm i.e it finds the shortest path from a given source node to all other nodes in the graph in $O(V + E log V )$

>On the other hand, **Floyd-Warshall** algorithm is a $O(V$<sup>$3$</sup>$)$ all-pairs shortest path algorithm i.e it finds the shortest distances between every pair of vertices in a given edge weighted directed graph.  

>In a graph with negative cycles (total sum of weights of edges in the cycle is negative), the problem of shortest path becomes **undefined**.\
But in a case where there are negative edge weights but no negative cycles, the shortest path problem is well-defined. **Dijkstra** algorithm does not work with negative egde weights though **Floyd-Warshall** algorithm does.

**Subproblem**

>Let $dist(i, j, k)$ denote the length of the shortest path from $i$ to $j$ in which only nodes ${1, 2, ..., k}$ can be used as **intermediates**.

**Optimal Substructure Property - How do we express this in terms of smaller subproblems ?**

>$dist(i,j,k) = min(dist(i,k,k-1) + dist(k,j,k-1), dist(i,j,k-1))$

>Here $dist(i,k,k-1) + dist(k,j,k-1)$ depicts the case when we decide to use the $k$<sup>$th$</sup> node, thus the path can be divided into two parts (i to k) and (k to j).

>$dist(i,j,k-1)$ depicts the case where we decide **not** to use the $k$<sup>$th$</sup>.

![ ](https://github.com/Github-Classroomtest/assignment-harshitagupta1512/blob/6ac55e48cbcc3993f3b9b4a4c49096cc64c34444/Pictures/Floyd_Marshall.png)

>The order of nodes $1,2,3...,k...,n$ can be any arbitrary order, but that order need to remain unchanged throughout the algorithm.

>For implementation, we must decide the loop order such that subproblems $dist(i,k,k-1)$,  $dist(k,j,k-1)$, $dist(i,j,k-1)$ are solved before $dist(i,j,k)$ in order to avoid any deadlocks. Thus the required order will be $k, i, j$.

**Algorithm**

```cpp
for i = 1 to n:
    for j = 1 to n:
        dist(i, j, 0) = infinity

for all (i, j) in E:
    dist(i, j, 0) = l(i, j) //directed edges

for k = 1 to n:
    for i = 1 to n:
        for j = 1 to n:
            dist(i,j,k) = min(dist(i,k,k-1) + dist(k,j,k-1), dist(i,j,k-1))
````

### Problem 8 - Independent Sets in Trees

> A tree is a connected acyclic graph. \
A tree with ‘n’ vertices has ‘n-1’ edges.

>The problem of independent sets in general graphs in NP-hard. 


**Problem**

>Find the largest independent set in given a tree T?.

>A subset $S$ of set of nodes $V$ is an independent set of graph $G = (V,E)$ if there are no edges between them.

>A **Bipartite Graph** is a graph whose vertices can be divided into two **independent sets**, U and V such that every edge (u, v) either connects a vertex from U to V or a vertex from V to U. \
In other words, for every edge (u, v), either u belongs to U and v to V, or u belongs to V and v to U. We also say that there is no edge that connects vertices of same set.

> Trees have this special property that if a decision is optimal for the subtrees, it is optimal for the tree as well.

**Subproblems**

>$I(u)$ = size of largest independent set of subtree hanging from u.

**How can we compute I(u) ?**

>We split the computation into two cases:
any independent set either includes `u` or it doesn't.

>$I(u)$ = $max$ ( 1 + $\sum I(w)$  where $w$ are the `grandchildren` of $u$ , $ \sum I(w) $, where $w$ are the `children` of $u$ ).

>The idea is that if a node is considered as a part of the largest independent set, then it's children cannot be a part of the LIS, but its grandchildren can be. 

>Time complexity of the above naive recursive approach is exponential. Though using DP we can solve the problem in O(n) where n is the number of nodes in the given tree. 

---