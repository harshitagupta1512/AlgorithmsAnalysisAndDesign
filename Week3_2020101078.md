# Week-3

## Lecture - 1

--------------------------------------------------------------------------------

## Fast Fourier Transform (FFT)

### Overview

In a lot of scientific applications (for example, signal processing),
multiplication of high degree polynomials is required. Here, we explore solving
the problem of multiplying two polynomials of degree "d" in reasonable time.

### Problem Statement

Given two "d" degree polynomials, compute their product.

### Brute Force / Naive Approach

A simple naive approach would be to apply the mathematical formula directly and
calculate the coefficients.

IF <br>
A(x) = a<sub>0</sub> + a<sub>1</sub>x + · · · + a<sub>d</sub>x<sup>d</sup> <br>
AND <br>
B(x) = b<sub>0</sub> + b<sub>1</sub>x + · · · + b<sub>d</sub>x<sup>d</sup> <br>
THEN <br>
C(x) = A(x) · B(x) = c<sub>0</sub> + c<sub>1</sub>x + · · · + c<sub>2d</sub>x<sup>2d</sup>.

A single coefficient c<sub>k</sub> of the final product can be expressed as: <br>
c<sub>k</sub> = a<sub>0</sub>b<sub>k</sub> + a<sub>1</sub>b<sub>k−1</sub> + · · · + a<sub>k</sub>b<sub>0</sub> i.e.  the sum of a<sub>i</sub>b<sub>k-1</sub> for i = 0 to k.

This clearly takes a time of O(d<sup>2</sup>) as we need to calculate 2d + 1
coefficients and each calculation takes O(d) time.

Can we do better here?

### An alternative representation

A line in geometry can be represented by either a one degree polynomial or
two distinct points. Similarly, a circle can be represented by either a two
degree polynomial or three distinct points on a plane.

Similarly, any d degree polynomial can be represented by d + 1 distinct points.

Hence, any polynomial <br>

A(x) = A(x) = a<sub>0</sub> + a<sub>1</sub>x + · · · + a<sub>d</sub>x<sup>d</sup> <br>
can also be represented as d +1 distinct points

A(x) = A(x<sub>0</sub>),A(x<sub>1</sub>), . . . A(x<sub>d</sub>)

### Multiplying with new representation

Suppose we directly had two polynomials in the new form.

Pick some points: x<sub>0</sub>, x<sub>1</sub> ... x<sub>n</sub> where n >= 2d +
1.

A(x) = A(x<sub>0</sub>),A(x<sub>1</sub>), . . . A(x<sub>n</sub>) <br>
AND <br>
B(x) = B(x<sub>0</sub>),B(x<sub>1</sub>), . . . B(x<sub>n</sub>) <br>
then their product can be simply calculated as

C(x) = A(x<sub>0</sub>)B(x<sub>0</sub>),A(x<sub>1</sub>)B(x<sub>1</sub>), . . .
A(x<sub>n</sub>)B(x<sub>n</sub>)

Points for the new polynomial can be found by just multiplying individual n
points of the polynomials. This approach clearly takes liner time (O(n)).

The remaining piece of the problem is to convert the coefficient form of the polynomial
to the new value form and vice versa.

Defining these two processes:

*   **Evaluation**: Conversion from coefficient representation to the value
    representation.

*   **Interpolation**: Conversion from value representation to the coefficient
    representation.

### Fast Fourier Transform

Naively speaking, we can do the evaluation by calculating the value of
polynomial at n different points. Each calculation will take linear time and
hence, the whole process of evaluation will take up the time of order of
O(n<sup>2</sup>).

The whole idea of the fast fourier transform is that we can improve upon this time
by intelligently selecting the points on which the value is to be calculated.

Let's separate the odd and even powers of the polynomial and express it in the form: <br>
A(x) = A<sub>e</sub>(x<sup>2</sup>) + xA<sub>o</sub>(x<sup>2</sup>)

Where A<sub>e</sub> represents the even number coefficients and A<sub>o</sub>
represents the odd number coefficients.

For example

A(x) = 3 + 4x + 6x<sup>2</sup> + 2x<sup>3</sup> + x<sup>4</sup> +
10x<sup>5</sup>

can be expressed as

A(x) = (3 + 6x<sup>2</sup> + x<sup>4</sup>) + x(4 + 2x<sup>2</sup> +
10x<sup>4</sup>).

If we choose the points to be positive-negative pairs:

A(x) = A<sub>e</sub>(x<sup>2</sup>) + xA<sub>o</sub>(x<sup>2</sup>)

AND

A(-x) = A<sub>e</sub>(x<sup>2</sup>) - xA<sub>o</sub>(x<sup>2</sup>)

This selection of points helps us use what we have already calculated and hence
reduces the computation. Quantitatively, we are able to reduce the problem of
getting n distinct values to getting (n/2) distinct values for 2 polynomials of
half degree along with some liner computation.

In other words, evaluating A(x) at n paired points ±x0, . . . , ±xn/2−1 reduces
to evaluating A<sub>e</sub>(x) and A<sub>o</sub>(x) (which each have half the
degree of A(x)) at just n/2 points: x<sub>0</sub><sup>2</sup>,
x<sub>1</sub><sup>2</sup>, .... x<sub>n/2 -1</sub><sup>2</sup>.

#### Time complexity:

The above method gives us the relation:

T(n) = 2T(n/2) + O(n)

which translates to a time complexity of O(nlogn).

#### But the trick doesn't work after the first iteration?

The plus-minus trick only works at the top level of the recursion. To recurse at
the next level, we need the n/2 evaluation points x<sub>0</sub><sup>2</sup>,
x<sub>1</sub><sup>2</sup>, .... x<sub>n/2 -1</sub><sup>2</sup> to be themselves
plus-minus pairs. But the squares can never be negative!

The solution, of course, is to use complex numbers.

#### Reverse Engineer the complex numbers

To get which complex numbers to use, we will reverse engineer. At the bottom,
the base case of the recursion, the element will be 1 which can be returned as
it is.

Hence, the level above it should contain the square root of 1. The next level
would contain the square roots of +1, -1. Clearly, the complex numbers to be
used are the solution of the equation z<sup>n</sup> = 1.

The n roots of unity are 1, w, w<sup>2</sup>, w<sup>3</sup> ....... w<sup>n -
1</sup>.

#### The FTT algorithm

```cpp
function FFT(A, ω)
Input: Coefficient representation of a polynomial A(x)
of degree ≤ n − 1, where n is a power of 2
ω, an nth root of unity
Output: Value representation A(ω^0)
), . . . , A(ω^n−1)
if ω = 1: return A(1)
express A(x) in the form Ae(x^2) + xAo(x^2)
call FFT(Ae, ω^2) to evaluate Ae at even powers of ω
call FFT(Ao, ω^2) to evaluate Ao at even powers of ω
for j = 0 to n − 1:
compute A(ω^j) = Ae(ω^2j) + ωjAo(ω^2j)
return A(ω^0), . . . , A(ω^n−1)
```

### Getting back the coefficients (Interpolation)

Amazingly, we can get back the coefficients from the value representation by
just using the FTT algorithm, but this time by using w<sup>-1</sup>.

```
Values = FTT (Coefficients, w)

Coefficients = FTT(Values, w^-1)
```

> Hence, we finally get an algorithm to calculate the product of two polynomials
> in O(nlogn) time.

## Lecture - 2

--------------------------------------------------------------------------------

## Greedy Algorithms

### The Minimum Spanning Tree Problem

Input - A undirected weighted graph G(V,E). \
Output - A tree T(V,E'), E' is a subset of E, that minimizes weight(T) =
summation of edge weights.

#### Kruskal's Algorithm (a greedy approach)

Start with an empty graph and repeatedly add the next lightest edge that doesn't
produce a cycle.

##### Proof for why Kruskal's algorithm always work correctly

The cut property forms the basis for the Kruskal's and Prim's algorithm for
finding MST. Infact, one can derive exponential number of similar algorithms to
find the MST using the cut property.

**The Cut Property** - Suppose edges X are part of a minimum spanning tree of
G = (V, E) . Pick any subset of nodes S for which X does not cross between S and
V − S , and let e be the lightest edge across this partition. Then X ∪ {e} is
part of some MST.

**Proof for the cut property**

> Let T be a minimum spanning tree that includes X, and assume that T does not
> contain the light edge e, since if it does, we are done. \
> We construct another MST T' that includes A U {e}. Let {u, v} be the ends of
> edge e. The edge e forms a cycle with the edges on the simple path from u to v
> in T. Since u and v are on opposite sides of the cut (S, V - S), at least one
> edge in T lies on the simple path p and also crosses the cut. Let e' be any
> such edge. The edge e' is not in A. Since e' is on the unique simple path from
> u to v in T , removing e' breaks T into two components. Adding e reconnects
> them to form a new spanning tree T' = T - {e'} + {e}. \
> T' is a tree since it has the same number of edges as T and a tree with n
> nodes always have n-1 edges. \
> T' is a minimum spanning tree, since e is a light edge crossing (S, V - S) and
> e' also crosses this cut, w(e) < w(e') Therefore, w(T') = w(T) - w(e') + w(e)
> < w (T).

**Pseudocode for Kruskal's Algorithm** 
```cpp 
MST_Kruskal(G,w)
    A = []
    for each vertex v in G.V
        Make_Set(v)
    sort the edges of G.E in increasing order by weight
    for each edge (u, v) in G.E
    if Find_Set (u) != Find_Set(v)
        A = A U {(u,v)}
        Union(u, v)
    return A
```` 
\
We start by making singleton sets with each node and initialise A as null set.
Then, we sort the edge list and traverse through it, for each edge where the end
points are in different sets, we add that edge to A and combine the sets.

**Disjoint Set Data Structure**

A set can be represented as a directed tree. The name of the set is same as the
root node.

```cpp
procedure Make_Set(x) //Create a singleton set containing just x.
    P(x) = x
    rank(x) = 0
````
Rank is the height of the sub-tree hanging from that node.
P is the parent pointer.

```cpp
function Find_Set(x) //Find the set to which x belongs
    while(x != P(x))
        x = P(x)
    return x
````

Considering the set as directed graphs Find_Set returns the root node of the graph/set in which x belongs.

```cpp
procedure Union(x, y) //The Union procedure merges the sets containing x and y.
    r_x = Find_Set(x)
    r_y = Find_Set(y)
    if r_x = r_y
        return
    if(rank (r_x)  > rank(r_y)):
        P(r_y) = r_x
    else:
        P(r_x) = r_y
        if(rank(r_x) = rank(r_y))
            rank(r_y) = rank(r_y) + 1
````
Make the root of the shorter tree point to the root of the taller one.

**Time Complexities**

>Makeset - O(1)
Find - O(log n)
A balanced tree with n nodes has log n height thus the maximum time required to find the root node by moving from parent to parent from any other node in the tree  = O(logn)
Union - O(logn)
Union procedure involves the just find procedure twice.
Overall - O((E + V)log V)

**Can we do better**
The first idea is to use a better sorting algorithm to sort the edges in less than O(Elog(E)) if possible.
The second idea is to perform the disjoint set procedures in less than O(log n) which can be achieved by Path Compression.
**Modifying find procedure** - During each find, when a series of parent pointers is followed up to the root of a tree, we will change all these pointers so that they point directly to the root.
```cpp
function Find(x)
    if(x!=P(x))
        P(x) = find(P(x))
    return P(x)
````
The time taken by a specific find operation is simply the number of pointers followed.
Overall complexity of modified find is  O(m log*n)

*Thus, Kruskal’s Algorithm using Disjoint-Sets data structure with path compression runs with the complexity of: “Sorting |E| elements” + O(|E| log*|V|)

----