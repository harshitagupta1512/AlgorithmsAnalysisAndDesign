# Week 4
### Lecture 1
-----
## Greedy Algorithms
**Basic Greedy Design**
>1. **Greedy Choice Property** \
    Can we know one-first-step towards the
    optimum solution ? i.e can we figure out the first step towards the optimal solution of our problem.

>2. **Optimum Substructure Property**\
    Once the first-step is taken, can we restate the rest of the problem as a smaller version of the original problem ?

>If both of the above mentioned properties are satisfied by a problem it can be optimally solved by the greedy approach.\
> We know the first step to the solution, by the greedy choice property. We can figure out the second step which will the first step of the subproblem (smaller version of the original problem) after the first step, (by the optimum substructure property), and similarly the third step and so on.\
> Thus inductively we can find the optimal solution of the problem via the greedy approach.  

## Activity Selection
**Problem** - There is a set of n activities a = ${ a_1, a_2, a_3, .... a_n}$, where each activity is represented by a half - open interval ${[s_i, f_i)}$, where ${s_i}$ is the start time and ${f_i}$ is the finish time. The activities are competing for a single resource. The goal is to select the largest possible set of non-overlapping (mutually compatible) activities.\
There can be multiple optimal solutions to this problem.

**Solution** - **The Greedy Choice Property**
>Which of the n activities is guaranteed to be in some optimum solution ?  
>The activity with the **least finish time** belongs to some optimum solution.  
>**Proof** - Let A be a maximum-size subset of mutually compatible activities from S. (One of the optimal solutions)\
>Let ${a_0}$ be the activity with minimum finish time among all activities.\
>Order activities in A in monotonically increasing order of finish time. Let ${a_k}$ be the first activity in A.\
>If ${a_k = a_0}$ , the proof is done (${a_0}$ is used in a maximum-size subset).\
>Otherwise, construct  B = A − ${a_k}$ U ${a_0}$ (replace $a_k$ by $a_0$) .\
>Activities in A are disjoint, ${a_k}$ is the first activity in A to finish, ${f_0 ≤ f_k}$, so ${a_0}$ does not overlap with anything else in A). Thus B is a maximum-size subset (another optimal solution to the problem ) too and $a_0$ belongs to B. Hence, proved.

**Optimum Substructure Property** 
>For the activities compatible, with the activity $a_0$ i.e activities that start after $a_0$ finishes.

**Pseudocode**
```cpp
ActivitySelector(s, f, n)
    A <- a[0]
    i <- 0
    for m = 1 to n - 1
        if s[m] >= f[i]
            A = A U {a[m]}
            i = m // a_i is the last activity added in the subset

    return A        
`````
## Huffman Codes

**Problem** - Given a string, a type of code made up of certain alphahbets. Find an optimal encoding to represent the code as a binary string of minimum possible length.

**Fixed-length Encoding** -  If number of distinct characters = k, each character can be represented as a binary string of length log<sub>2</sub>k.

Can there possibly be a better encoding than this?  

**Variable-length Encoding**
>Prefix-free property: No codeword can be a prefix of another codeword.\
 If we try to represent different characters using binary strings of variable length then we must keep the prefix-free property in mind otherwise decoding a binary string can become cumbersome.\
>So if we have binary codes of different characters using the prefix-free property they can be represented using a full binary tree.
```
                        .

           0                         1
    00          01            10           11
            010   011     100   101     110   111 
````
where the leaf nodes of the tree represent the encoding of each symbol/character in the code. (if an internal node represents a symbol, it's leaf node cannot represent any other symbol(prefix-free property) if not, then there is no need to draw it's leaf nodes thus it is the leaf itself.

**Precise Problem** - Given the frequencies $f_1 ,f_2 , ... ,f_n$ of n symbols, we want a tree whose leaves each correspond to a symbol and which minimizes the overall length of the encoding.
Cost of tree = Summation ($f_i$*(depth of i<sup>th</sup> symbol in the tree)).

**Greedy Choice Property** - The two symbols with the smallest frequencies must be at the bottom of the optimal tree, as children of the lowest internal node. Otherwise, swapping these two symbols with whatever is
lowest in the tree would improve the encoding.

Define the frequency of any internal node to be the sum of the frequencies of it's descendant leaves. The cost of a tree is the sum of the frequencies of all leaves
and internal nodes, except the root.

Total length of binary equivalent string of our original code will be the sum of the frequency of each node where frequency of each leaf node is same as the frequency of the corresponding symbol in the code. And the frequency of a parent node is the sum of the frequencies of it's child nodes.

**The optimal substructure property** - 
Any tree in which $f_1$ and $f_2$ are sibling-leaves
has cost $(f_1 + f_2)$ plus the cost for a tree with
(n-1) leaves of frequencies $(f_1 + f_2), f_3, f_4 , ..., f_n$

**Pseudocode**
```cpp
procedure Huffman(f)
Input: An array f[1 · · · n] of frequencies
Output: An encoding tree with n leaves

let H be a priority queue of integers, ordered by f

for i = 1 to n:
     insert(H, i)
for k = n + 1 to 2n − 1:
    i = deletemin(H), j = deletemin(H)
    create a node numbered k with children i, j
    f [k] = f [i] + f [j]
    insert(H, k)
````

### ENTROPY

>Given a probability distribution, say the probabilities of three horses finshing a race at position first/second/third/other. We can say that the most predictable horse is the one with maximum compressibility. That is if we make a string out of the past performance of the horse first/second/third/other and then find the minimum possible length of a binary string used to represent it, the horse requiring the minimum number of bits will be most compressible and thus most predicatble.\
The randomness of the probability distribution can be mea-
sured by the extent to which it is possible to compress data drawn from that distribution. More compressible ≡ Less random ≡ More predictable.
Entropy of the distribution is a measure of how much randomness it contains.
eg. A biased coin is more predictable than a fair coin, and thus has lower entropy. As the bias becomes more pronounced, the entropy drops toward zero.

----