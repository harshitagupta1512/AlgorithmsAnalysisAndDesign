# Week-2
## Lecture - 1
---
### Multiple ways of solving tractable problems
There can be multiple solutions to a problem some of them being better than the others
#### Example 1 - FIND THE N<sup>th</sup> FIBONACCI NUMBER F<sub>n</sub>
Fibonacci Series - 0,1,1,2,3,5,8,13,21,34,55
##### Algorithm #1 (Recursive Method)
```cpp
long long int fib(int n) {

if (n==0)
    return 0;
if(n==1)
    return 1;
return fib(n-1) + fib(n-2); 

}
```
Recursive Relation
T(n) = T(n-1) + T(n-2) + 3 for n>1
This implementation does a lot of repeated work for example, it will calculate fib(n-2) twice for both fib(n) and fib(n-1). 

##### Algorithm #2 (Iterative Method)
```cpp
long long int fib(int n){

if(n == 0)
    return 0;

long long int f[n + 1];
f[0] = 0;
f[1] = 1;

for(int i=2 ; i <= n ; i++)
    f[i] = f[i-1] + f[i-2];

return f[n - 1];
}
````
Iteration completely removes the overlapping computations.
Though it appears to be a linear time algorithm but the complexity of this approach is O(n<sup>2</sup>) because it involves addition of arbitarily large integers (F<sub>n</sub> is about 0.694 bits long ) and addition of two n-bit numbers takes O(n).

##### Algorithm #3 (Matrix Exponentiation)

$$\begin{pmatrix} F_n \\ F_{n+1} \end{pmatrix}
= 
\begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}^n
\cdot
\begin{pmatrix} F_0 \\ F_1 \end{pmatrix}$$

Solving this involves O(logn) matrix multiplications using the optimised (recursive) approach to calculate powers of matrices.

Time complexity for this approach will be T(n) = O(M(n)logn), where M(n) is the complexity of multiplying two n-bit integers.

##### Algorithm #4 (Apply the direct formula)

$$F_n = \frac{1}{\sqrt{5}}(\frac{1+\sqrt{5}}{2})^n - \frac{1}{\sqrt{5}}(\frac{1 - \sqrt{5}}{2})^2$$

Having an irrational number in the formula arouses accuracy issues.

#### Example 2 - ALGORITHMS FOR MULTIPLYING LARGE INTEGERS 

##### Algorithm #1 (primary school solution)

Single digit multiplications, additions and shift operations.
Time Complexity - O(n<sup>2</sup>)

##### Algorithm #2 (Karatsuba Algorithm)

Using Divide and Conquer approach.
Time Complexity - $O(n^{1.585})$. 

To multiply two n-digit integers:
1. Add two $1/2*n$ digit integers.
2. Multiply three $1/2$ n-digit numbers.
3. Add, subtract, and shift $1/2$ n-digit integers to obtain result. 

There are algorithms with complexity better than $O(n^{1.585})$ and the research for the best solution (with the proof of optimality) is still open.


## Lecture - 2
--- 
### Divide and Conquer

**Divide** the problem into a number of subproblems that are smaller instances of the same problem.

**Conquer** the subproblems by solving them recursively. If the subproblem sizes are small enough, however, just solve the subproblems in a straightforward manner.

**Combine** the solutions to the subproblems into the solution for the original problem.

#### Master theorem for solving recurrences
The master method is used to solve recurrences of the form T(n) = aT(n/b) + f(n), where a >= 1, b > 1 are constants and f(n) is an asymptotically positive function.

The recurrence describes the running time of an algorithm that divides a problem of size n into 'a' subproblems, each of size n/b, where a and b are positive constants. The 'a' subproblems are solved recursively, each in time T(n/b). The function f(n) encompasses the cost of dividing the problem and combining the results of the subproblems.

We compare the function f(n) with the function n<sup>log<sub>b</sub>a</sup>.

Intuitively, the larger of the two functions
determines the solution to the recurrence.

If the function n<sup>log<sub>b</sub>a</sup> is larger, then the solution is O(n<sup>log<sub>b</sub>a</sup>).

If, as in case 3, the function f(n) is the larger, then the solution is O(f(n)).

If the two functions are the same size, we multiply by a logarithmic factor, and the solution is  O(n<sup>log<sub>b</sub>a</sup> logn).

#### Merge Sort - a O(n logn) Comparison sort

**Divide** -  Split the array into two equal halves. This step just involves computing the middle of the array which takes constant time so it contributes O(1) to T(n).

**Conquer** - Recursively sort each half. This step contributes 2T(n/2) to the total running time.

**Combine** - Merge the two sorted sub-arrays. Merge procedure on a n-element subarray takes time O(n).

```cpp
MergeSort(A,low,high)
if low < high
    mid = (low + high) / 2
    MergeSort(A,low,mid)
    MergeSort(A,mid+1,high)
    Merge(A,low,high)
````

Thus the total running time of Merge Sort is given by
T(n) = 2T(n/2) + O(1) + O(n) = 2T(n/2) + O(n)

Using master theorem we can find the solution to this recurrence relation a = 2, b = 2, f(n) = n, so n<sup>log<sub>b</sub>a</sup> = n = f(n), which implies T(n) = O(n logn).

Most of the work in done by merge, which doesn’t start, up to the recursion gets down to singleton sublists.

#### Ω(nlogn) - Lower bound for comparison based sorting

Every sorting algorithm is a series of comparisons between two array elements. Consider a decision tree (full binary tree) where each node represents a comparison. Say the root node compares a[i] and a[j], if a[i]>a[j] the algorithm proceeds to left otherwise to the right. Total number of distinct comparisons between n elements equals n! which implies total number of nodes in our decision tree = n!. Any approach that sorts the array is a path from root node to one of the leaf nodes

#### Matrix Multiplication

##### Algorithm #1 (Naive approach)
Time Complexity - O(n<sup>3</sup>)
```cpp
MatrixMultiplication(A, B)
n = A.rows
let C be a new n*n matrix
for i = 1 to n
    for j = 1 to n
        c_ij = 0
        for k = 1 to n
            c_ij += a_ik*b_kj
return C
````

##### Algorithm #2 (A simple divide and conquer algorithm)

In each divide step we divide our n*n matrix into four n/2 * n/2 matrices.

$X = \begin{bmatrix} A & B \\ C & D \end{bmatrix}, Y = \begin{bmatrix} E & F \\ G & H \end{bmatrix}$

$
XY = \begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} E & F \\ G & H \end{bmatrix} = \begin{bmatrix} AE + BG & AF + BH \\ CE + DG & CF + DH \end{bmatrix}.$

This approach involves 8 matrix multiplications of size n/2 each, as a result the total running time for multiplying two matrices of size n is given by
T(n) = 8*T(n/2) + O(n<sup>2</sup>), where O(n<sup>2</sup>) is the complexity of matrix addition.

By master theorem the solution of this recurrence will be T(n) = O(n<sup>log<sub>2</sub>8</sup>) = O(n<sup>3</sup>)

##### Algorithm #3 (Strassen’s Algorithm)

Instead of performing eight recursive multiplications of n/2 * n/2 matrices, Strassen performed only seven. Though the cost of eliminating one matrix multiplication will be several new additions of n/2 * n/2 matrices.

Divide the two input n*n matrices into a total of 8 n/2 * n/2 matrices. 
Using this 8 matrices form another 7 matrices as follows - 

1. $P_1 = A*(F - H)$ 
2. $P_2 = (A+B)*H$ 
3. $P_3 = (C+D)*E$
4. $P_4 = D*(G - E)$
5. $P_5 = (A+D)*(E+H)$
6. $P_6 = (B - D)*(G + H)$
7. $P_7 = (A - C)*(E + F)$

XY  =$\begin{bmatrix} P_5 + P_4 - P_2 + P_6 & P_1 + P_2 \\ P_3 + P_4 & P1 + P_5 - P_3 - P_7 \end{bmatrix},$

This approach involves 7 matrix multiplications of size n/2 each, as a result the total running time for multiplying two matrices of size n is given by
T(n) = 7*T(n/2) + O(n<sup>2</sup>), where O(n<sup>2</sup>) is the complexity of matrix addition.

By master theorem the solution of this recurrence will be T(n) = O(n<sup>log<sub>2</sub>7</sup>) = O(n<sup>2.8</sup>)

#### Median Finding

Given a list of n numbers, compute the median.

##### Algorithm #1 (Direct approach)
Merge sort and return the mid-ranked element\
Time Complexity  = O(nlogn)

##### Algorithm #2 (divide and conquer)
For any number v, split list named $S$ into 3 categories:
1. Elements smaller than $v$: $S_L$
2. Elements equal to $v$: $S_V$
3. Elements greater than $v$: $S_R$

Recursion Steps

Selection(S,k) = 

1. If $k \leq |S_L|$: $Selection(S_L,k)$
2. If $|S_L|$ $<$ $k \leq |S_L|+|S_v|$: $v$
3. If $k$ $>$ $|S_v|+|S_L|$: $Selection(S_R,k-|S_L|-|S_v|)$

How to choose v ?

1. Median-of-Medians (Deterministic Approach)
    
    * Divide $n$ elements into groups of 5. 
    * Find median of each of the 5 groups.
    * Fine the median $x$ of the $n/5$ medians.

This is beneficial because symmetrically speaking, the number of elements that are $<x$ is atleast $3n/10-6$.

2. Randomized Approach (Choose v randomly)

Time Complexity - O(n)

---










